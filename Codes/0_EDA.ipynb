{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "EDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This file was developed as part of the project reported in the paper below. If you use our work, please cite our paper.\n",
        "\n",
        "- Title: UlyssesNER-Br: a Corpus of Brazilian Legislative Documents for Named Entity Recognition\n",
        "- Authors: Hidelberg O. Albuquerque, Rosimeire Costa, Gabriel Silvestre, Ellen Souza, Nádia F. F. da Silva, Douglas Vitório, Gyovana Moriyama, Lucas Martins, Luiza Soezima, Augusto Nunes, Felipe Siqueira, João P. Tarrega, Joao V. Beinotti, Marcio Dias, Matheus Silva, Miguel Gardini, Vinicius Silva, Andrré C. P. L. F. de Carvalho and Adriano L. I. Oliveira.\n",
        "- In: International Conference on the Computational Processing of Portuguese ― PROPOR 2022 (March 2022)"
      ],
      "metadata": {
        "id": "oUwBB_C1zn8U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCsMKI2mVfs2"
      },
      "source": [
        "# Exploratory Data Analysis - Phases 1 and 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rrBA9kuJ6mf"
      },
      "source": [
        "!pip install sklearn-crfsuite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6TR3Sr6Vfs7"
      },
      "source": [
        "import operator\n",
        "import os\n",
        "import random\n",
        "import functools\n",
        "import collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import sklearn_crfsuite\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from nltk.tag.hmm import HiddenMarkovModelTrainer\n",
        "from sklearn.model_selection import KFold\n",
        "from itertools import chain\n",
        "from nltk.corpus import PlaintextCorpusReader \n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "random.seed(1999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsPUEa7JVfs-"
      },
      "source": [
        "#DIR is the path to a folder containing all files in .conll format\n",
        "DIR = '/caminho/'\n",
        "all_files = os.listdir(DIR)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Got0_0WVfs_"
      },
      "source": [
        "# convert files in .conll format into nested lists with the following hierarchy:\n",
        "# Level 1: Sentences\n",
        "# Level 2: Tuples of type (Token, Tag)\n",
        "\n",
        "def process_conll_file(location:str)->list:\n",
        "    with open(location, \"r\") as f:\n",
        "        data = f.read()\n",
        "    data = data.split(\"\\n\\n\")\n",
        "    data = list(map(lambda x:x.split(\"\\n\"), data))\n",
        "    data.pop()\n",
        "    data = list(map(lambda x:[operator.itemgetter(*[0, -1])(y.split(\" \")) for y in x], data))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htQk9QKlVftA"
      },
      "source": [
        "# merge the outputs of the process_conll_file function, now instead of the list containing the sentences of a single file, it contains the sentences of all files in DIR.\n",
        "def combine_files(locations:list)->list:\n",
        "    extended = []\n",
        "    for f in locations:\n",
        "        f = DIR + f\n",
        "        extended.extend(process_conll_file(f))\n",
        "    return extended"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGidCYIwVftB"
      },
      "source": [
        "#main1\n",
        "all_data = combine_files(all_files)\n",
        "random.shuffle(all_data)\n",
        "f\"Número Total de Documentos: {len(all_data)}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWoSvX1fVftD"
      },
      "source": [
        "## Estatísticas sobre as sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuI74vjMVftD"
      },
      "source": [
        "#box plot of the distribution of sentences.\n",
        "tamanhos_sent = []\n",
        "for d in all_data:\n",
        "  tamanhos_sent.append(len(d))  \n",
        "plt.boxplot(tamanhos_sent, labels=[\"\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reHF3biKVftE"
      },
      "source": [
        "#Point estimation of the average number of tokens per sentence with confidence interval.\n",
        "media_sent = np.mean(tamanhos_sent)\n",
        "std_sent = np.std(tamanhos_sent)\n",
        "z_alpha = 1.96\n",
        "rng = (z_alpha * std_sent) / np.sqrt(len(tamanhos_sent))\n",
        "print(f\"Número Médio de Tokens por sentença: {media_sent}\")\n",
        "print(f\"Intervalo de Confiança (alpha = 5%): {(media_sent-rng, media_sent+rng)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQJGHJV3VftE"
      },
      "source": [
        "## Estatísticas sobre os tokens e tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tH03R9cVftC"
      },
      "source": [
        "#removes the hierarchy of sentences, that is, the tokens become independent of each other.\n",
        "def to_list(data:list)->list:\n",
        "    return functools.reduce(operator.iconcat, data, [])\n",
        "\n",
        "#returns two lists: One containing all dataset tokens and another containing their respective tags respecting the order.\n",
        "def split_words_n_tags(data:list)->tuple:\n",
        "    words, tags = map(list, zip(*data))\n",
        "    return words, tags "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiIZ_gh4VftF"
      },
      "source": [
        "all_pairs = to_list(all_data)\n",
        "all_words, all_tags = split_words_n_tags(all_pairs)\n",
        "f\"Número de tokens no dataset: {len(all_words)}\", f\"Tamanho do Vocabulário: {len(set(all_words))}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIckIlgCVftG"
      },
      "source": [
        "#Removal of I- and B- prefixes (to improve the model)\n",
        "all_tags_limpo = []\n",
        "for tag in all_tags:\n",
        "    if tag==\"O\":\n",
        "        all_tags_limpo.append(tag)\n",
        "    elif tag.startswith(\"B-\") or tag.startswith(\"I-\"):\n",
        "        all_tags_limpo.append(tag[2:])\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "f\"Número de Categorias ou Tipos: {len(set(all_tags_limpo))}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kSoxxEhVftH"
      },
      "source": [
        "#tags counter \n",
        "tag_hist = collections.Counter(all_tags_limpo)\n",
        "tag_hist "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwVEPHalVftH"
      },
      "source": [
        "#Point estimation of the proportions of each tag\n",
        "prop = {key:(val/sum(tag_hist.values())) * 100 for key, val in tag_hist.items()}\n",
        "prop = dict(sorted(prop.items(), key=lambda item: item[1]))\n",
        "for key, val in prop.items():\n",
        "    print(f\"{key} & {val:.2f}\\%\\\\\\\\\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZzSJZO4VftH"
      },
      "source": [
        "#bar chart with distributions\n",
        "tag_hist.pop(\"O\")\n",
        "tag_hist = dict(sorted(tag_hist.items(), key=lambda item: item[1]))\n",
        "keys = tag_hist.keys()\n",
        "vals = tag_hist.values()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.barh(range(len(keys)), tag_hist.values())\n",
        "ax.set_yticks(range(len(keys)))\n",
        "ax.set_yticklabels(keys)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvzkiz2BVftI"
      },
      "source": [
        "eps = z_alpha * np.sqrt(1/(4*len(all_words)))\n",
        "print(f\"Erro ao estimar a proporção (abordagem conservativa, alpha = 5%): {(eps * 100):.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}